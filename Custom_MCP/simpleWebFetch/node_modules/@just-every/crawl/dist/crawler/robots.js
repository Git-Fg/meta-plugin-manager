import { fetchStream } from './fetch.js';
const robotsCache = new Map();
export async function getRobotsChecker(origin, userAgent = '*') {
    const cached = robotsCache.get(origin);
    if (cached)
        return cached;
    try {
        const robotsUrl = new URL('/robots.txt', origin).href;
        const robotsTxt = await fetchStream(robotsUrl, {
            timeout: 5000,
            userAgent,
        });
        // Dynamic import for CommonJS module
        const robotsParserModule = (await import('robots-parser'));
        const robotsParser = robotsParserModule.default || robotsParserModule;
        const robots = robotsParser(robotsUrl, robotsTxt);
        robotsCache.set(origin, robots);
        return robots;
    }
    catch {
        // If robots.txt fetch fails, create a permissive checker
        const permissive = {
            isAllowed: () => true,
            getCrawlDelay: () => undefined,
        };
        robotsCache.set(origin, permissive);
        return permissive;
    }
}
export async function isAllowedByRobots(url, userAgent = '*') {
    try {
        const { origin } = new URL(url);
        const checker = await getRobotsChecker(origin, userAgent);
        return checker.isAllowed(url, userAgent);
    }
    catch {
        return true; // Default to allowed if any error
    }
}
export async function getCrawlDelay(url, userAgent = '*') {
    try {
        const { origin } = new URL(url);
        const checker = await getRobotsChecker(origin, userAgent);
        return checker.getCrawlDelay(userAgent) || 0;
    }
    catch {
        return 0;
    }
}
//# sourceMappingURL=robots.js.map