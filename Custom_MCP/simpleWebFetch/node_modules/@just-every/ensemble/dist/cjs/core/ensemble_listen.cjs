"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.ensembleListen = ensembleListen;
exports.createAudioStreamFromMediaStream = createAudioStreamFromMediaStream;
const model_provider_js_1 = require("../model_providers/model_provider.cjs");
function normalizeAudioSource(source) {
    if (source instanceof ReadableStream) {
        return source;
    }
    if (typeof source === 'object' && source !== null && Symbol.asyncIterator in source) {
        return new ReadableStream({
            async start(controller) {
                try {
                    for await (const chunk of source) {
                        controller.enqueue(chunk);
                    }
                    controller.close();
                }
                catch (error) {
                    controller.error(error);
                }
            },
        });
    }
    if (typeof source === 'function') {
        const iterable = source();
        return normalizeAudioSource(iterable);
    }
    if (source instanceof ArrayBuffer || source instanceof Uint8Array) {
        const data = source instanceof ArrayBuffer ? new Uint8Array(source) : source;
        return new ReadableStream({
            start(controller) {
                controller.enqueue(data);
                controller.close();
            },
        });
    }
    throw new Error(`Unsupported audio source type: ${typeof source}`);
}
async function* ensembleListen(audioSource, agent, options = {}) {
    const streamOptions = { ...options, stream: true };
    const audioFormat = options.audioFormat || {
        sampleRate: 16000,
        channels: 1,
        encoding: 'pcm',
    };
    const startEvent = {
        type: 'transcription_start',
        timestamp: new Date().toISOString(),
        format: audioFormat.encoding || 'pcm',
        audioFormat: audioFormat,
    };
    yield startEvent;
    const model = await (0, model_provider_js_1.getModelFromAgent)(agent, 'transcription');
    let provider;
    try {
        provider = (0, model_provider_js_1.getModelProvider)(model);
    }
    catch (error) {
        const errorEvent = {
            type: 'error',
            timestamp: new Date().toISOString(),
            error: `Failed to initialize provider for model ${model}: ${error instanceof Error ? error.message : 'Unknown error'}`,
        };
        yield errorEvent;
        return;
    }
    if (!provider.createTranscription) {
        const errorEvent = {
            type: 'error',
            timestamp: new Date().toISOString(),
            error: `Provider for model ${model} does not support transcription`,
        };
        yield errorEvent;
        return;
    }
    let audioStream;
    try {
        audioStream = normalizeAudioSource(audioSource);
    }
    catch (error) {
        const errorEvent = {
            type: 'error',
            timestamp: new Date().toISOString(),
            error: `Failed to normalize audio source: ${error instanceof Error ? error.message : 'Unknown error'}`,
        };
        yield errorEvent;
        return;
    }
    const startTime = Date.now();
    let fullTranscript = '';
    let currentTurnText = '';
    const allTurns = [];
    try {
        const transcriptionGenerator = provider.createTranscription(audioStream, agent, model, streamOptions);
        for await (const event of transcriptionGenerator) {
            if (event.type === 'transcription_turn_delta' && event.delta) {
                fullTranscript += event.delta;
                currentTurnText += event.delta;
            }
            if (event.type === 'transcription_turn_complete') {
                const turnEvent = {
                    ...event,
                    text: currentTurnText,
                };
                yield turnEvent;
                if (currentTurnText.trim()) {
                    allTurns.push(currentTurnText.trim());
                }
                currentTurnText = '';
            }
            else {
                yield event;
            }
        }
        if (currentTurnText.trim()) {
            allTurns.push(currentTurnText.trim());
        }
        const duration = (Date.now() - startTime) / 1000;
        const completeEvent = {
            type: 'transcription_complete',
            timestamp: new Date().toISOString(),
            text: allTurns.length > 0 ? allTurns.join('\n') : fullTranscript,
            duration: duration,
        };
        yield completeEvent;
    }
    catch (error) {
        console.error('[ensembleListen] Error during transcription:', error);
        const errorEvent = {
            type: 'error',
            timestamp: new Date().toISOString(),
            error: error instanceof Error ? error.message : 'Transcription failed',
        };
        yield errorEvent;
    }
}
function createAudioStreamFromMediaStream(mediaStream, audioContext) {
    const ctx = audioContext || new AudioContext({ sampleRate: 16000 });
    const source = ctx.createMediaStreamSource(mediaStream);
    const processor = ctx.createScriptProcessor(4096, 1, 1);
    return new ReadableStream({
        start(controller) {
            processor.onaudioprocess = e => {
                const float32Audio = e.inputBuffer.getChannelData(0);
                const int16Audio = new Int16Array(float32Audio.length);
                for (let i = 0; i < float32Audio.length; i++) {
                    const s = Math.max(-1, Math.min(1, float32Audio[i]));
                    int16Audio[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
                }
                controller.enqueue(new Uint8Array(int16Audio.buffer));
            };
            source.connect(processor);
            processor.connect(ctx.destination);
        },
        cancel() {
            processor.disconnect();
            source.disconnect();
        },
    });
}
//# sourceMappingURL=ensemble_listen.js.map